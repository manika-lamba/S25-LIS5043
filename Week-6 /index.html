<!DOCTYPE html>
<html lang="en"><head>
<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/tabby.min.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.555">

  <meta name="author" content="Dr.&nbsp;Manika Lamba">
  <title>Module 4.2: Information Retrieval Evaluation</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="styles.css">
  <link href="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Module 4.2: Information Retrieval Evaluation</h1>
  <p class="subtitle">LIS 5043: Organization of Information</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Dr.&nbsp;Manika Lamba 
</div>
</div>
</div>

</section>
<section>
<section id="introduction" class="title-slide slide level1 center">
<h1>Introduction</h1>
<aside class="notes">
<p>In this week’s module, we’re going to talk about information evaluation and how we evaluate systems. I’m going to introduce you to some of the metrics and some of the ideas that have been used in the past as well as talk about some of the usability issues that you might also address when evaluating information retrieval systems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="approaches-to-ir-evaluation" class="slide level2">
<h2>Approaches to IR Evaluation</h2>
<ul>
<li><code>Efficiency</code>
<ul>
<li>How economically the system is achieving its objective
<ul>
<li>IR system storage capacity, speed, costs</li>
</ul></li>
</ul></li>
<li><code>Effectiveness</code>
<ul>
<li>Level up to which the given system attains its stated objective(s)</li>
<li>Usefulness of IR results, relevance of results, benefits to users</li>
</ul></li>
<li><code>Usability/user studies</code></li>
</ul>
<aside class="notes">
<p>There are several different approaches to information retrieval evaluation, and I’ve listed a few of them here. There are others that can be used to evaluate systems and to evaluate systems in different contexts. Probably the most prevalent that are in still in use today are those related to efficiency, effectiveness, and usability or user studies.</p>
<p>Efficiency is defined as “how economically the system is achieving its objective or goals”. For every information retrieval system, or for any system in general, there are usually a set of achievable objectives. For example, in an IR system, evaluation would be to determine whether or not the system is retrieving documents back to the user effectively; if the system’s storage capacity is enough so that the users are getting good, quick retrieval; whether or not the inverted indexes are working correctly, etc. So, aspects like storage capacity, speed, and cost figure into the efficiency equation.</p>
<p>Effectiveness, is defined as “the level to which the given system attains its stated objectives”. So, again, you would look at whether or not the system is attaining the goals most of the time, part of the time, etc. We measure this by looking at different kinds of data–for example, the usefulness of IR results. Are the system searches bringing back accurate results? Are the results relevant? And do the results match the search queries? And then are the results beneficial for our users?</p>
<p>The third approach includes ‘usability’ and/or ‘user studies.’ These are studies where we examine people as they use systems and measure certain aspects of their system use, such as how many clicks they make to accomplish a task, how much time they spend in each task, or what features they use, etc.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="evaluation-criteria" class="slide level2">
<h2>Evaluation Criteria</h2>
<ul>
<li>Extent to which the system meets both the expressed and latent needs of its users</li>
<li>The reasons for failure to meet user needs</li>
<li>Cost-effectiveness of searches made by users versus those of intermediaries</li>
<li>Are services offered appropriate for user needs? Sources available appropriate?</li>
<li>Changes needed to meet users needs?
<ul>
<li>Search features/output, interface design, scope of collection, indexing issues, etc.</li>
</ul></li>
</ul>
<aside class="notes">
<p>There are obviously lots of different criteria that we can use for evaluation. I have listed a few that are addressed on this slide. For example, the extent to which a system meets both the expressed and latent needs of its users. Remember that an information system does not exist in isolation. Usually when a library is getting ready to implement a new system, they’ve already talked to their users or they’ve done surveys to find out the needs of their users so they know the express needs, but also the latent needs can be determined by evaluating what’s going on with the system. Are people using the system? What features are they using? You can look at search logs. You can use logs to determine if the system is meeting both the expressed and latent needs of the users.</p>
<p>Another evaluation criteria is the reasons for failure if the system is not meeting the user needs. In this case, you could track searches and system use. You could talk to the users themselves. You could look at the use logs to see how often people are using certain features of the system. Are they coming back to the system? These are some of the ways you can evaluate this criteria.</p>
<p>You can also look at the cost effectiveness of searches made by users versus those of intermediaries, like reference librarians. You can have users and intermediaries run the same search in the system and look at the strategies they use, but also look at the time factor and the learning factors associated with that experience.</p>
<p>Are services offered appropriate to your users’ needs? Are sources available that are appropriate as well? You might be evaluating an existing system and talk to users about whether or not they use your system. Are the services appropriate for the user? Are they meeting the needs of that community? But also are there appropriate sources available within the system itself?</p>
<p>One other aspect is to determine what changes might be needed to meet user needs. You might look very closely at elements like search features. What’s available? Is there a basic and advanced search? What kind of output is sent back to users as a result of the search? How is the interface designed? What’s the scope of the collection? Even looking at indexing issues can be part of this evaluation criteria.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="evaluation-levels-level-i" class="slide level2">
<h2>Evaluation Levels: Level I</h2>
<ul>
<li><code>Evaluation of Effectiveness</code>
<ul>
<li>consideration of user satisfaction</li>
</ul></li>
</ul>
<p>A. Cost criteria</p>
<pre><code>1. Monetary cost to user    
2. Other, less tangible, costs
    a. Effort involved in learning to use system
    b. Effort involved in access/ use
    c. Effort involved in retrieving documents
    d. Form of output provided by the system</code></pre>
<aside class="notes">
<p>Some researchers also look at evaluation from different levels, so let’s talk about some of these measures as well.</p>
<p>At level one, we are evaluating effectiveness or consideration of user satisfaction with a particular retrieval system. Within this area we look at cost criteria, both the monetary cost to the user and other, less tangible costs to the user, such as efforts involved in learning to use the system, efforts involved in access and use of the system, what it takes to retrieve documents in the system, and whether the form of output provided by the system is appropriate for what the user may need to accomplish.</p>
<p>For example, do users like just getting a one sentence description or do they want a small paragraph so that they can evaluate the document? How useful are, say, the bibliographic entries that a user gets back when searching in a database system? And then perhaps if a user can you link out to the article or do they have to follow a chain of related links in order to get to the article that they’re looking for? Cost criteria is not just based on the monetary cost to a user, but also these less tangible costs as well.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="evaluation-levels-level-i-1" class="slide level2">
<h2>Evaluation Levels: Level I</h2>
<p>B. Time criteria</p>
<pre><code>1. Time from request to retrieval of references
2. Time from request to retrieval of documents
3. Other, e.g., waiting to use system—availability of computer/speed of connection</code></pre>
<p>C. Quality criteria</p>
<pre><code>1. Coverage of database
2. Completeness of output (recall measure)
3. Relevance of output (precision measure)
4. Novelty output
5. Completeness and accuracy of data
6. Access to full text documents or surrogates</code></pre>
<aside class="notes">
<p>We also look at time criteria, and time, of course, is related to many different aspects, for example, time from request to retrieval of references–so, from the time the user starts their search, runs their search, and then receives output from the system,–time from the request to the retrieval of documents–and what is the time involved. There are other metrics here as well, such as the availability of a computer or computers to use and waiting to use the system. So, if this system is in a public library environment, and people are waiting on the average half an hour to forty-five minutes or longer to use the system, you might consider purchasing more computers if possible.</p>
<p>Speed of connection isn’t as much of an issue now as it used to be, but again, there are some systems that are only available through a slower line, as opposed to T1 lines that we see in a lot of universities and in a lot of public libraries as well. But these again are criteria that could factor into time.</p>
<p>There are also quality criteria that we use to evaluate systems, such as the coverage of the database itself. What formats of articles are available? How many different types of disciplines are included? Is it a news database, or is it very subject specific, such as library literature? And also related to coverage is the year span. Can you get every possible document back to the beginning of publication, or are things embargoed for a time period, so something published over the last six to eight months might not be yet available in the online database. Issues of coverage are a problem as well.</p>
<p>Completeness of output, is dealing with recall. If you’ll remember recall is the ability of the system to retrieve only the relevant documents, and we’ll talk about that more in a minute. But completeness of output includes what is returned back to the users. And then on the inverse of that, of course, is relevance of output. Is the system retrieving all of the documents that are relevant or match our search query, or what we call ‘precision’?</p>
<p>There’s also another search measure called ‘novelty output,’ or whether there are documents that are new to the user, or if there are document that the user has not seen before.</p>
<p>The completeness and accuracy of the data is also related to the quality of the data. Are the users getting accurate information? Are there typos in the citations? Are there incomplete or broken links? Aspects like completeness and accuracy are related to quality.</p>
<p>A further aspect of quality is whether or not the user has access to the full document or just to a surrogate, which I know can be very frustrating for users. They’ll go to the trouble of finding a citation they want, and then they’ll find out that the particular system doesn’t have access to the full text document. This is happening less and less in our systems nowadays, but again, think of your own experiences using OU’s databases. Oftentimes you’ll find a citation you want, and then find that you have no way to retrieve the document unless you follow it all the way through and request it first via interlibrary loan. So again, this is another criteria related to quality.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="evaluation-levels-level-ii" class="slide level2">
<h2>Evaluation Levels: Level II</h2>
<ul>
<li><code>Evaluation of Cost-Effectiveness</code>
<ul>
<li>user satisfaction related to internal system efficiency and cost considerations</li>
</ul></li>
</ul>
<p>A. Unit cost per relevant citation retrieved</p>
<p>B. Unit cost per new (previously unknown) relevant citation retrieved</p>
<p>C. Unit cost per relevant document retrieved</p>
<aside class="notes">
<p>At level two we would look at cost-effectiveness, which is the user satisfaction related to internal system efficiency and cost considerations. We’re not just looking at cost from a monetary view, we’re looking at those intangible costs, such as how long it takes to learn a system or how frustrated a user becomes learning and/or using the system. But we can look at unit cost per relevant citations retrieved.</p>
<p>We can look at unit cost per new or previously unknown document, or novelty items. Citations that are retrieved, and then also unit cost per relevant document retrieved are other factors we can evaluate. We don’t look at cost effectiveness as much as we used to, but this factor is definitely another way in which you can evaluate the effectiveness of the system.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="evaluation-levels-level-iii" class="slide level2">
<h2>Evaluation Levels: Level III</h2>
<ul>
<li>Cost-Benefit Evaluation
<ul>
<li>value of system balanced against costs of operating or of using it</li>
</ul></li>
</ul>
<aside class="notes">
<p>At evaluation level number three, we would be conducting a cost-benefit evaluation, This type of evaluation is a little bit more difficult because this measure has a lot of subjective aspects. We would evaluate the value of the system balanced against the cost of operating or using it.</p>
<p>This form of evaluation is a combination of the factors we just discussed such as the quality criteria, the time criteria, the scope of a database, how long it takes to learn it, what search features are available and are they and the interface designed for effective use by the user, can users determine if there’s a controlled vocabulary with the system and how to use it if it’s available.</p>
<p>These factors are balanced against a value the user places on the search experience. Was it a good experience? Did they find what they were looking for? Did they find the system easy to use? And other value-related metrics.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="traditional-ir-effectiveness-measures" class="slide level2">
<h2>Traditional IR Effectiveness Measures</h2>
<ul>
<li>Recall Assesses
<ul>
<li>Ability of system to retrieve all the relevant items it contains</li>
</ul></li>
<li>Precision assesses
<ul>
<li>Ability of system to retrieve only the relevant items</li>
</ul></li>
</ul>
<aside class="notes">
<p>There are also traditional information retrieval measures that are used to evaluate systems. In Chapter 14 of Chowdhury, you will read about some large-scale studies, such as MEDLARS and STAIRS and TREC. These studies, as well as those conducted today in many contexts, use effectiveness measures of recall and precision. Let’s talk about each of these in turn and how they can be useful in evaluating the effectiveness of IR systems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="recall" class="slide level2">
<h2>Recall</h2>
<p><code>RECALL = Relevant Documents in a IR Set/All relevant documents in the database</code></p>
<ul>
<li>A measure of how good a system is at retrieving all the relevant documents</li>
<li>Inversely related to precision</li>
<li>Dependent upon the users’ expectations and objectives</li>
<li>Difficult to estimate</li>
<li>Need to know the number of relevant documents in the entire collection</li>
</ul>
<aside class="notes">
<p>Recall is the measure of how good a system is at retrieving all the relevant documents that are part of that system’s collection. The equation used to evaluate recall is included on the slide for you. We do know that in most cases and in most retrieval systems, recall is inversely related to precision. So, if you have a high level of recall, you have a low level of precision.</p>
<p>So, you would need to know what’s more important to the user–high levels of precision, meaning they are getting exactly on topic documents, or high levels of recall, which means the user wants everything that might be even tangentially associated with the topic they are researching. Of course, recall and precision are dependent upon our users’ expectations and objectives for their searches, and it is in this way that this evaluation measure becomes very subjective. It’s also really difficult to estimate because we need to know the number of the relevant documents in the entire collection in order to run recall measures.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="section" class="slide level2">
<h2><img data-src="images/clipboard-3837408382.png" style="width:100.0%"></h2>
<aside class="notes">
<p>An example of recall would be if we have a collection that has three items on a particular topic in the collection, and in this case I’m going to say these items are on frogs. I have run a query and the system retrieves two relevant documents and one document that is not relevant.</p>
<p>So, this as a .66% recall rate because two of the three relevant documents were retrieved in the search.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="precision" class="slide level2">
<h2>Precision</h2>
<p><code>PRECISION =  Relevant documents in a retrieved set/All documents in the retrieved set</code></p>
<ul>
<li>Measures how good the system is at “not” retrieving non-relevant documents</li>
<li>Dependent upon user’s expectations and objectives</li>
</ul>
<aside class="notes">
<p>Precision, then, measures how good the system is at not retrieving those non-relevant documents–in other words, bringing back documents that are only related to your query and nothing that’s irrelevant. This again, is a measurement that is dependent upon users’ expectations and objectives. The user is the only one that can really evaluate whether or not a document is useful to them. And again, it’s difficult to measure precision because we have to know how many documents in the collection are on particular topics or related to particular topics in a database.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="section-1" class="slide level2">
<h2><img data-src="images/clipboard-3990879776.png" style="width:100.0%"></h2>
<aside class="notes">
<p>So, in terms of precision, again, let’s use our same collection of documents. Within that collection there are three documents we know are related to frogs. Our query then is also ‘frogs,’ and the system only returns one precise document, or one document that is precisely related to our query. And also it retrieves one document that is not related to our query. So, we have the equation of one document that is relevant with a return of two documents. So, our precision in this case is .50.</p>
<p>Some of our large-scale studies run precision and recall measures by setting up document sets in which the researchers know every topic that’s included in that document set. And then they run queries that have been developed for those document sets against the collection using different kinds of search tools and different kinds of algorithms that the researchers developed in order to test precision and recall. The TREC studies are still conducted today. Researchers compete developing and using new algorithms to test the precision and recall of their system.</p>
<p>They also share information with each other about how to improve retrieval metrics, such as precision and recall in IR systems. There are a lot of different TREC studies you can read online. And TREC deals not just with English language; it does also include cross language information retrieval as well, which is called CLIR. There is an entire community for just CLIR and competitions related to CLIR as well. TREC includes IR studies and competitions for not just text based documents, but also for images and multimedia and blogs and social networks using many different information retrieval systems. It is really interesting to see the problems and the issues that the researchers are trying to resolve by developing better retrieval mechanisms.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="other-measurement-issues" class="slide level2">
<h2>Other Measurement Issues</h2>
<ul>
<li><p><code>What about RELEVANCE?</code></p>
<ul>
<li>Defined: ???? Cannot be precisely defined.</li>
<li>Subjective</li>
<li>Unique between individual and a specific document</li>
<li>Can be dependent upon many factors</li>
<li>Can change over time, even within same IR session</li>
</ul></li>
<li><p>More practical or topical approach to relevance?</p>
<ul>
<li>Usefulness?</li>
<li>Salience?</li>
</ul></li>
</ul>
<aside class="notes">
<p>What about this term ‘relevance’? We have a large body of research in our field related to relevance. It’s not easy to give you a precise definition of it because it is very subjective in nature. Relevance basically means that a retrieved document has been assessed by an individual to be something that they find on topic–that it is related to their search query.</p>
<p>We can take this even a step further and say that it’s useful to them, but determining relevance is unique between the individual and the specific document or a specific retrieval set. So, the individual has to determine whether or not it’s relevant. Relevance can be determined depending upon many different factors. A user’s evaluation of relevance also changes over time, even within the same IR session. So, relevance is very dynamic in nature. A lot of LIS/IR research also looks at degrees of relevance within information seeking and information search sessions.</p>
<p>We have been rethinking the idea of relevance to view it more as a practical or topical approach, such as usefulness or salience, meaning that a user can apply the information they learn within the document to the problem they are trying to solve or to something they are trying to work with. So, again, LIS/IR researchers have entertained this idea of more practical or topical approaches to relevance. But again, it’s very subjective, and only the user can determine whether or not something is relevant.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="other-measures-to-consider" class="slide level2">
<h2>Other Measures to Consider</h2>
<ul>
<li><p><code>Fall Out Ratio</code>: proportion of non-relevant items returned in a search to relevant items</p></li>
<li><p><code>Generality Ratio</code>: proportion of relevant items in a collection for a given query</p></li>
</ul>
<aside class="notes">
<p>Some other measures to consider are fall out ratio and generality ratio. Fall out ratio is the proportion of non-relevant items that are returned in the search to relevant items. So, we’re not just looking at the relevant items that a person found. We’re also interested in how many are also non-relevant.</p>
<p>Generality ratio is the proportion of relevant items in a collection to a given query. You can see right away this one is a little harder to evaluate because we then have to know how many objects in our collection are related to specific topics and why or why not they were not retrieved when a search was conducted.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="indexing-factors-affecting-ir-performance" class="slide level2">
<h2>Indexing Factors Affecting IR Performance</h2>
<p><code>Human Indexing</code></p>

<img data-src="images/clipboard-2704263523.png" class="quarto-figure quarto-figure-center r-stretch" style="width:60.0%"><aside class="notes">
<p>We can also consider human factors, which can be associated with the indexing of the documents within the system, such as the human’s types and levels of knowledge, as well as affective and cognitive variables that affect humans on a daily basis.</p>
<p>So, related to indexing, in an earlier module we discussed the measure of inter-indexer consistency and inter-indexer inconsistency. LIS/IR research has shown that there’s really at the highest level about 30% of consistency between any two indexers that are creating subject terms for the same document. So, consistency can be an issue. Also, subject expertise and indexing experience play into whether or not the index terms chosen are appropriate, accurate, and complete. If a person has a high degree of subject expertise, or domain knowledge, in a particular area, such as chemistry or ethnobotany, then they’re probably going to know the literature of the field and they’re going to know the appropriate levels of specificity and exhaustivity when they’re indexing.</p>
<p>There are also, as I said, types of knowledge, for example, I just mentioned domain knowledge and the indexer’s understanding of the domain itself, but also their search experience. Generally people that have a lot of experience with different search systems and have looked at the different structures within them and also use different techniques for searching are better indexers because they understand the effects that their decisions play on retrieval.</p>
<p>There are also factors related to motivation or emotional state of the indexer. We’ve done some research looking at motivation level of indexers and some of the errors they may if they’re unhappy in their job, or the emotional state of individuals and how that might impact their indexing as well. We don’t have any recent studies in this area, but we do know that motivation and emotional state do have an effect on indexing, so the decisions an indexer makes when choosing terms to represent a subject of a document can impact retrieval.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="indexing-factors-affecting-ir-performance-1" class="slide level2">
<h2>Indexing Factors Affecting IR Performance</h2>
<p><code>System Factors</code></p>

<img data-src="images/1.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<p>We had talked in an earlier lecture about some of the system indexing factors that affect information retrieval performance. These are also evaluation measures or metrics that we use when we evaluate information retrieval systems. Some of the system factors are related to, say, index language used and the indexing assignment within the system.</p>
<p>One aspect we can evaluate is if the indexer/cataloger is choosing the appropriate vocabulary to use within that system. And then also if they have chosen the appropriate levels of specificity in terms that they are using to assign subject terms to particular documents. Also of consideration is the level of coordination of terms within the system. Do you have similar concepts being represented with the same subject terms, for example? Or is the specificity level consistent throughout the system? And is the specificity level consistent with the users who are interacting with the system?</p>
<p>In terms of indexing assignment, we evaluate if the level of exhaustivity is appropriate. For example, how are we representing the items within the system? Are we representing a broad, overall subject, or are we representing multiple topics and subtopics for each document? Also, how many different terms are we using to represent the subjects within those different documents? Specificity is also part of this. Are we using the accurate level of specificity in our terms that meets the level of specificity in the documents themselves? Are accurate terms being assigned to represent those subjects? Are we representing the subjects themselves accurately and completely within the document?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="other-measures-to-consider-1" class="slide level2 smaller">
<h2>Other Measures to Consider</h2>
<ul>
<li><code>Usability Measures</code>
<ul>
<li>How are users interacting with system?</li>
<li>What features used? Why or why not used?
<ul>
<li>Different levels of search available?</li>
<li>Are all features used or does system have unneeded features?</li>
</ul></li>
<li>Return to use same system again?</li>
<li>Feedback mechanism? (relevance or other?)</li>
<li>Interface design measures</li>
<li>ADA Compliance issues</li>
</ul></li>
</ul>
<aside class="notes">
<p>The third type of metric that I mentioned on one of the earlier slides is called ‘usability.’ There are lots of books written on usability; there are websites devoted to usability. Usability looks more specifically at how users are interacting with information retrieval systems and with webpages or websites.</p>
<p>Usability studies look at a range of different things, like what features are being used on the site, and then the evaluator tries to determine why a feature is or is not being used. We also look at whether or not the system has extraneous features, or unneeded features on a website. Did you know that nobody ever clicks on the link up in the far left corner, because honestly, they don’t see it? We look at different levels of searching that are available. Does the site provide a site map? Does the site provide both a basic and an advanced search? Does the system have different functions for different types of users?</p>
<p>Evaluators also look at whether or not people come back and use the systems again. We can track the return rate with IP tracking online, but we can also track it in libraries if we require people to log on to systems with their password and user id; we can check it in our databases by just looking at search logs.</p>
<p>Another measure we look at in usability is the feedback mechanisms within the interface and retrieval system. What kind of messages do users get when they conduct a search? Remember that we talked that IR is a little bit like a conversation, where the user has their own language and the system has its language and somehow they have to be able to communicate with each other.</p>
<p>So, there’s lots of different feedback that happens between the user and the system, including messages like the return of search query results and how they’re displayed back to the user or whether or not a system has a spell check mechanism and asks the user, “Did you mean this instead?” Or whether or not the system will just correct the spelling for us. So, those messages are really important. But we also evaluate factors such as how easy it for a user to begin a new search. Is there a box that says “Back to Search” or “Edit Search” or some feature that allows the user to then go back to their search and revise it?</p>
<p>We also look at different design aspects of interfaces when evaluating usability. Are the colors appropriate? Are there flashing elements that are distractive and sometimes very dangerous, especially for special needs individuals? There are many different design-related measures that can be part of usability measures.</p>
<p>We also look at ADA compliance issues, or the American Disability Act compliance issues. There are specific criteria that are set out to make sure that users with disabilities have a good experience when using a system. For example, websites at a federal or a nationally funded level, or even some states have ADA compliance regulations. There are different criteria that websites have to follow. So, for example, if your website has images, it has to also include mouse over with the text tags that pop up.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="other-measures-to-consider-2" class="slide level2 smaller">
<h2>Other Measures to Consider</h2>
<ul>
<li><code>Satisfaction with system</code>
<ul>
<li>Representation scheme known?</li>
<li>Syntax for entering query evident?</li>
<li>Possible IR technique(s) known?</li>
<li>Output issues</li>
<li>Comfort Level
<ul>
<li>Ease of use/access</li>
<li>Are instructions evident? Easy to use? Easy to understand?</li>
</ul></li>
<li>Speed of input/output</li>
</ul></li>
</ul>
<aside class="notes">
<p>One additional measure that we evaluate in usability but also in other evaluation studies I mentioned is the user’s satisfaction with the system. Satisfaction can obviously come from a lot of different interactions that they have had while using the system and also from the system structure itself.</p>
<p>For example, we might evaluate how transparent the representation aspects of search systems are to the user:</p>
<ul>
<li>Do they know what the representation scheme is? For example, what subject heading or controlled vocabularies are being used within the system?</li>
<li>Do they have access to the system’s controlled vocabulary?</li>
</ul>
<p>This is most often provided through a thesaurus feature, where a user can click a button or a link and then input their search term, and the system will convert it or give them a list of terms that the specific system is using to represent that topic.</p>
<p>Also, what fields are searchable within the system?</p>
<p>This is not as much of an issue anymore as it used to be since we have dropdown boxes that allow us to filter our searches or limit our searches into only specific inverted indexes. So, for example, when you click on the dropdown box and you can choose the author or the title field, what you’re in essence doing is limiting your search to that inverted index field.</p>
<p>Another factor of satisfaction is related to the user’s understanding of how to enter their query into the system.</p>
<p>For example,</p>
<ul>
<li>Is there a specific syntax that has to be used for entering the query, and if so, is it explained to the user?</li>
<li>And how easy is it for them to implement?</li>
</ul>
<p>Another aspect is whether or not the system makes the possible information retrieval techniques known.</p>
<ul>
<li><p>Can you conduct Boolean searching, and if so, how does a user do so in this particular system?</p></li>
<li><p>Are there ways to truncate search terms, or use the root of a term with a specific character assigned by the system, so that the system searches not just for that specific word, but for the root and all of its forms?</p></li>
</ul>
<p>There are also output issues:</p>
<ul>
<li>What type of output is returned back to the user?</li>
<li>Did the output only include a hyperlink out to a search result, or is there some little blurb that explains what they’re going to look at?</li>
<li>How easily can the user then access the document once they have found it through search results?</li>
</ul>
<p>There’s also comfort level issues:</p>
<ul>
<li>How easy is it to use and to access the right database, and once you find something, how quickly can the user retrieve the document?</li>
<li>Is it in full text, or is the user taken to a citation and not to the article? Or</li>
<li>If the user needs to retrieve the item from interlibrary loan, can you link to the interlibrary loan interface from the search results or bibliographic record page?</li>
<li>What format is the item available in?</li>
<li>Is it in HTML, or is it in PDF?</li>
</ul>
<p>So, there are lots of different ways of looking at ease of use and access:</p>
<ul>
<li>Also, are instructions and FAQs evident?</li>
<li>And how easy to use are these features?</li>
<li>Are the FAQs understandable?</li>
<li>What audience(s) are they written for?</li>
<li>Do the FAQs include the information that you really need to learn about using the system?</li>
</ul>
<p>And then lastly, speed of input and output:</p>
<ul>
<li>How long does it take you to use this system?</li>
<li>How long does it take you to learn to use this system?</li>
<li>And are FAQS I mentioned earlier evident and easy to find and use?</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="other-issues-to-consider" class="slide level2">
<h2>Other Issues to Consider</h2>
<ul>
<li>Electronic versus Print</li>
<li>Subscription versus Web</li>
<li>Copyright Issues</li>
<li>Access Issues</li>
<li>Price and Subscription Issues</li>
<li>Vendor, Compatibility and Customization Issues</li>
</ul>
<aside class="notes">
<p>There are also some additional issues to consider, and these are related to how the information is available, how the user can retrieve it from the system, such as is it electronic or print. If I’m using a library OPAC, can I link out to an electronic copy or an eBook? Or do I need to be able to physically access the materials by being in the library? Or can I link out to interlibrary loan or Sooner Express and request that it be pulled from the shelf for me to pick up?</p>
<p>Also, there are issues of subscription databases versus the web. They both work very differently as you know.</p>
<p>There are copyright issues of how that information can be used once it’s retrieved, as well as those access issues I mentioned earlier about how do we go from a retrieval result directly to the article or item itself and what levels and mechanisms are involved in that.</p>
<p>Also, price and subscription issues. This is something that libraries deal with a lot – reevaluating how to spend their money for extremely expensive databases.</p>
<p>And then we might also have vendor and compatibility and customization issues that can be part of system evaluation. For example, will our system work well with other systems that we’re also using in the library? Can we customize the system to fit our user needs better? Vendor relations can also be considered an aspect of system evaluation. A library or information professional has to be able to relate this information to the vendor. For example, how easy is it to get changes to the system made? Is it possible to make the various systems interoperate with each other.</p>
<p>This lecture has introduced you very briefly to some of the approaches to system evaluation that you may use in your library or information organization. Please refer to the readings for this module to learn more about these measures. If you have any questions, of course post your question and thoughts to the class discussion board for Module 4.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="also-of-interest" class="slide level2">
<h2>Also of Interest</h2>
<ul>
<li>Be sure you read Chowdhury Ch-14 to learn more about the large scale IR evaluation studies conducted through the years (MEDLARS, <a href="https://trec.nist.gov/">TREC</a>, etc.) and how results from these tests have improved IR</li>
</ul>
<aside class="notes">
<p>If you are interested in learning more about the large-scale IR evaluation studies I mentioned in this lecture refer to the Chowdhury Ch-14, and also search the Web to find more information on the <a href="https://trec.nist.gov/">TREC competition</a></p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="quarto-auto-generated-content">
<p><img src="images/ou.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="index_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="index_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="index_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="index_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="index_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="index_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="index_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"theme":"whiteboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: "separate-page",

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: true,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>